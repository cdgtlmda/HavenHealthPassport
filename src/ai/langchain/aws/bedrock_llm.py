"""Bedrock LLM Integration for LangChain.

This module provides LangChain-compatible LLM wrappers for AWS Bedrock models,
specifically optimized for medical and refugee healthcare use cases.
"""

from typing import Any, Dict, Iterator, List, Optional

from botocore.exceptions import ClientError
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.schema import Generation, LLMResult

try:
    from langchain.schema import GenerationChunk  # type: ignore[attr-defined]
except (ImportError, AttributeError):
    try:
        from langchain.schema.output import GenerationChunk
    except (ImportError, AttributeError):
        # If GenerationChunk is not available, use Generation as fallback
        GenerationChunk = Generation
from pydantic import Field

from src.services.bedrock_service import BedrockService
from src.utils.logging import get_logger

logger = get_logger(__name__)


class BedrockLLM(LLM):
    """LangChain-compatible wrapper for AWS Bedrock LLMs."""

    model_name: str = Field(default="anthropic.claude-3-opus-20240229-v1:0")
    temperature: float = Field(default=0.7, ge=0.0, le=1.0)
    max_tokens: int = Field(default=1000)
    top_p: float = Field(default=0.9, ge=0.0, le=1.0)
    stop_sequences: Optional[List[str]] = None

    # Medical-specific parameters
    medical_mode: bool = Field(default=True)
    include_reasoning: bool = Field(default=True)

    # Service instance
    _bedrock_service: BedrockService

    class Config:
        """Pydantic config."""

        extra = "forbid"

    def __init__(self, **data: Any) -> None:
        """Initialize Bedrock LLM."""
        super().__init__(**data)
        self._bedrock_service = BedrockService()

    @property
    def _llm_type(self) -> str:
        """Return type of LLM."""
        return "bedrock"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get identifying parameters."""
        return {
            "model_name": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "medical_mode": self.medical_mode,
        }

    def _prepare_prompt(self, prompt: str) -> str:
        """Prepare prompt with medical context if needed."""
        if self.medical_mode:
            medical_context = """You are a medical AI assistant helping with refugee healthcare.
Always prioritize patient safety and medical accuracy.
Consider cultural sensitivity and language barriers in your responses.
"""
            return f"{medical_context}\n\n{prompt}"
        return prompt

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call out to Bedrock's generate endpoint.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.

        Returns:
            The string generated by the model.
        """
        # Prepare the prompt
        prepared_prompt = self._prepare_prompt(prompt)

        # Combine stop sequences
        all_stop_sequences = []
        if self.stop_sequences:
            all_stop_sequences.extend(self.stop_sequences)
        if stop:
            all_stop_sequences.extend(stop)

        try:
            # Call Bedrock service
            response_text, metadata = self._bedrock_service.invoke_model(
                prompt=prepared_prompt,
                model_id=self.model_name,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                top_p=self.top_p,
                stop_sequences=all_stop_sequences if all_stop_sequences else None,
            )

            # Log token usage if available
            if run_manager and metadata.get("usage"):
                run_manager.on_llm_new_token("", verbose=self.verbose)

            # Return the response text directly
            return response_text

        except Exception as e:
            logger.error(f"Error calling Bedrock: {str(e)}")
            raise

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """Run the LLM on the given prompts."""
        generations = []
        for prompt in prompts:
            text = self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
            generations.append([Generation(text=text)])
        return LLMResult(generations=generations)

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        """Stream the LLM on the given prompt.

        Note: Streaming support depends on the Bedrock model.
        """
        # Prepare the prompt
        prepared_prompt = self._prepare_prompt(prompt)

        # Combine stop sequences
        all_stop_sequences = []
        if self.stop_sequences:
            all_stop_sequences.extend(self.stop_sequences)
        if stop:
            all_stop_sequences.extend(stop)

        try:
            # Use the streaming method from BedrockService
            stream_kwargs: Dict[str, Any] = {
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "top_p": self.top_p,
            }

            if all_stop_sequences:
                stream_kwargs["stop_sequences"] = all_stop_sequences

            # Stream responses from Bedrock
            for chunk in self._bedrock_service.stream_invoke(
                prompt=prepared_prompt, model_id=self.model_name, **stream_kwargs
            ):
                # Notify callback manager if available
                if run_manager:
                    run_manager.on_llm_new_token(chunk, verbose=self.verbose)

                yield GenerationChunk(text=chunk)

        except (ClientError, ValueError, KeyError, AttributeError) as e:
            logger.error(f"Error streaming from Bedrock: {str(e)}")
            # Fall back to non-streaming if streaming fails
            logger.info("Falling back to non-streaming response")
            response = self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
            yield GenerationChunk(text=response)


class BedrockClaudeLLM(BedrockLLM):
    """Specialized Bedrock LLM for Claude models with medical optimizations."""

    model_name: str = Field(default="anthropic.claude-3-opus-20240229-v1:0")

    # Claude-specific parameters
    use_xml_tags: bool = Field(default=True)
    reasoning_style: str = Field(default="step-by-step")

    def _prepare_prompt(self, prompt: str) -> str:
        """Prepare prompt with Claude-specific formatting."""
        base_prompt = super()._prepare_prompt(prompt)

        if self.use_xml_tags and self.medical_mode:
            # Add XML tags for better Claude performance on medical tasks
            tagged_prompt = f"""<medical_query>
{base_prompt}
</medical_query>

Please provide your response with clear reasoning."""

            if self.include_reasoning:
                tagged_prompt += """

<reasoning>
[Explain your medical reasoning step-by-step]
</reasoning>

<answer>
[Provide your final answer]
</answer>"""

            return tagged_prompt

        return base_prompt


def get_bedrock_llm(
    model_name: str = "anthropic.claude-3-opus-20240229-v1:0",
    temperature: float = 0.7,
    max_tokens: int = 1000,
    medical_mode: bool = True,
    **kwargs: Any,
) -> BedrockLLM:
    """Get appropriate Bedrock LLM based on model name.

    Args:
        model_name: Name of the Bedrock model
        temperature: Sampling temperature
        max_tokens: Maximum tokens to generate
        medical_mode: Whether to use medical optimizations
        **kwargs: Additional model parameters

    Returns:
        Configured Bedrock LLM instance
    """
    # Use Claude-specific class for Claude models
    if "claude" in model_name.lower():
        return BedrockClaudeLLM(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            medical_mode=medical_mode,
            **kwargs,
        )

    # Default Bedrock LLM for other models
    return BedrockLLM(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
        medical_mode=medical_mode,
        **kwargs,
    )
